{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "\n",
    "The initial model is a basic RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import projd\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import importlib\n",
    "from keras.layers import Dense, SimpleRNN, Input\n",
    "from keras.models import Model\n",
    "import keras\n",
    "\n",
    "from IPython.display import SVG # visualize model\n",
    "from keras.utils.vis_utils import model_to_dot # visualize model\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# for importing local code\n",
    "src_dir = str(Path(projd.cwd_token_dir('notebooks')) / 'src') # $PROJECT_ROOT/src\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "import config\n",
    "import datagen\n",
    "importlib.reload(config)\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "GEN_STRIDE = 20 # for generation of overlapping text substrings\n",
    "EPOCHS=10\n",
    "BATCH_SIZE=32\n",
    "SAMPLE_MODEL_EPOCHS = 20 # sample model output (generated text) every N epochs.\n",
    "VOCAB_SIZE = 256\n",
    "ALPHA_REGULARIZER = 0.05\n",
    "n_a = 128 # number of hidden units\n",
    "n_y = VOCAB_SIZE # predict next character\n",
    "n_x = VOCAB_SIZE # input current character\n",
    "n_t = 40 # sequence length\n",
    "\n",
    "model_name = f'model01_rnn_{n_a}_{n_t}_{GEN_STRIDE}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validation Sets\n",
    "\n",
    "- Load Preprocessed Datasets\n",
    "- Divide into training and validation sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vectorized Datasets\n",
    "\n",
    "The jokes, names, and book datasets are preprocessed in the following manner:\n",
    "\n",
    "- The text of the dataset is combined, lowercased, and white-space normalized.\n",
    "- The cleaned text is split into overlapping strings of length n_t.  \n",
    "  They overlap by (n_t - GEN_STRIDE) characters.\n",
    "- The characters are converted to integers (via ISO Latin 1 encoding) and then 1-hot encoded\n",
    "- The y/output sequences are the x sequences shifted over one, with a space character appended \n",
    "  to the end to make the sequence length the same.\n",
    "- The result is an X shape of (m, n_t, n_x) and a Y shape of (m, n_t, n_y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = datagen.get_tensors(n_t, VOCAB_SIZE, GEN_STRIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide Datasets Into Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seed = 1\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.25, random_state=train_seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_x=n_x, n_y=n_y, n_a=n_a, n_t=n_t):\n",
    "    '''\n",
    "    n_x: number of input features.  The size of the vocabulary.  Each char is one-hot encoded\n",
    "    n_y: number of output features.  The same as n_x for next character prediction.\n",
    "    n_a: number of hidden units in rnn layer\n",
    "    n_t: the length of each sequence.\n",
    "    '''\n",
    "    ## the input is a sequence of characters that have been one-hot encoded.\n",
    "    x_input = Input(shape=(n_t, n_x))\n",
    "    x = SimpleRNN(n_a, return_sequences=True, kernel_regularizer=keras.regularizers.l2(ALPHA_REGULARIZER))(x_input)\n",
    "    y = Dense(n_y, activation='softmax', kernel_regularizer=keras.regularizers.l2(ALPHA_REGULARIZER))(x)\n",
    "    \n",
    "    model = Model(inputs=x_input, outputs=y)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "model = build_model()\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Model\n",
    "\n",
    "- Add callbacks to save model every 20 epochs and to log performance stats every epoch, so we have the results saved somewhere for charting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = config.models_dir  /  (model_name +'_{epoch:02d}.h5')\n",
    "def get_model_path(model_name, epoch):\n",
    "    return config.models_dir  /  (model_name + f'{epoch:02d}.h5')\n",
    "\n",
    "# Callbacks include ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "# Save the model\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    str(model_path), monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, \n",
    "    mode='auto', period=1)\n",
    "# Stop when validation loss stops improving\n",
    "early_cb = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "# Save logs to logfile\n",
    "log_path = config.logs_dir / (model_name + '_log.csv')\n",
    "log_cb = keras.callbacks.CSVLogger(str(log_path), separator=',', append=False)\n",
    "\n",
    "history = model.fit(x_train, x_train, epochs=10, batch_size=BATCH_SIZE, validation_data=(x_val, y_val), \n",
    "                    callbacks=[checkpoint_cb, log_cb])\n",
    "#model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"acc\"])\n",
    "plt.plot(metrics[\"val_acc\"])\n",
    "plt.legend(['Training Accuracy', \"Validation Accuracy\"])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(metrics[\"loss\"])\n",
    "plt.plot(metrics[\"val_loss\"])\n",
    "plt.legend(['Training Loss', \"Validation Loss\"])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text Periodically During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_for_epochs(model_name, epochs):\n",
    "    for epoch in epochs:\n",
    "        path = model_path(model_name, epoch)\n",
    "        model = keras.models.load_model(path)\n",
    "        text = generate_text(model)\n",
    "        print('Epoch:', epoch)\n",
    "        print('Generated Text:')\n",
    "        print(text)\n",
    "\n",
    "def generate_text(model):\n",
    "    for i in range(length):\n",
    "        model.predict()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
