{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 10\n",
    "\n",
    "This model is the kitchen sink: a 1D Conv layer then an GRU layer, then another GRU layer then a Dense layer with a softmax activation, trained to predict the next character from the preceeding characters.  The dataset is Pride and Prejudice by Jane Austen, which contains around 680,000 characters.  The sequence length is short (20 chars) and the training set size is large (150k samples).  No dropout is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "import projd\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import importlib\n",
    "from keras.layers import Dense, SimpleRNN, Input, Conv1D, LSTM, GRU\n",
    "from keras.models import Model\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import SVG # visualize model\n",
    "from keras.utils.vis_utils import model_to_dot # visualize model\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# for importing local code\n",
    "src_dir = str(Path(projd.cwd_token_dir('notebooks')) / 'src') # $PROJECT_ROOT/src\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "import datagen\n",
    "import load\n",
    "importlib.reload(datagen)\n",
    "importlib.reload(load)\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "data_dir = Path('/data2/uvm_deep_learning_homework2')\n",
    "models_dir = data_dir / 'models'\n",
    "logs_dir = data_dir / 'logs'\n",
    "\n",
    "GEN_STRIDE = 3 # for generation of overlapping text substrings\n",
    "EPOCHS=100\n",
    "BATCH_SIZE=128\n",
    "NUM_GEN_TEXT_SAMPLES = 1 # number of generated text samples to create per epoch sampled\n",
    "GEN_SAMPLE_LEN = 80 # length of generated text samples\n",
    "SEQ_LEN = 20 # sequence length\n",
    "NUM_SEQS = 150000 # maximum number of sequences used for training\n",
    "VOCAB_SIZE = 256\n",
    "\n",
    "n_a = 128 # number of hidden units\n",
    "n_a2 = 64 # of filters in convolutional layer\n",
    "\n",
    "model_name = f'model_10_conv_lstm_{n_a}_{SEQ_LEN}_{GEN_STRIDE}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validation Sets\n",
    "\n",
    "- Load Preprocessed Datasets\n",
    "- Divide into training and validation sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vectorized Datasets\n",
    "\n",
    "The jokes, names, and book datasets preprocessed in the following manner:\n",
    "\n",
    "- The text of the dataset is combined, lowercased, and white-space normalized.\n",
    "- The cleaned text is split into overlapping strings of length SEQ_LEN.  \n",
    "  They overlap by (SEQ_LEN - GEN_STRIDE) characters.\n",
    "- The characters are converted to integers (via ISO Latin 1 encoding) and then 1-hot encoded\n",
    "- The y value corresponding to every sequence in x is the one-hot encoded character immediately \n",
    "  following the sequence in the text \n",
    "- The result is an X shape of (m, SEQ_LEN, VOCAB_SIZE) and a Y shape of (m, VOCAB_SIZE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = datagen.get_normalized_text(choice='pride', data_dir=data_dir)\n",
    "x, y = datagen.text_to_tensors(text, SEQ_LEN, GEN_STRIDE, VOCAB_SIZE, num_seqs=NUM_SEQS)\n",
    "\n",
    "# Confirm that the shape looks right.\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_x=VOCAB_SIZE, n_y=VOCAB_SIZE, n_a=n_a, n_a2=n_a2, n_t=SEQ_LEN):\n",
    "    '''\n",
    "    n_x: number of input features.  The size of the vocabulary.  Each char is one-hot encoded\n",
    "    n_y: number of output features.  The same as n_x for next character prediction.\n",
    "    n_a: number of hidden units in rnn layer\n",
    "    n_a2: number of hidden units in conv layer\n",
    "    n_t: the length of each sequence.\n",
    "    '''\n",
    "    ## the input is a sequence of characters that have been one-hot encoded.\n",
    "    x_input = Input(shape=(n_t, n_x))\n",
    "    x = x_input\n",
    "    x = Conv1D(n_a2, (3,), activation='relu')(x)\n",
    "    x = GRU(n_a, return_sequences=True)(x)\n",
    "    x = GRU(n_a)(x)\n",
    "    y = Dense(n_y, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=x_input, outputs=y)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "model = build_model()\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Model\n",
    "\n",
    "- Add callbacks to save model every 20 epochs and to log performance stats every epoch, so we have the results saved somewhere for charting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks include ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "# Save the model\n",
    "model_path = models_dir  /  (model_name +'_{epoch:02d}.h5')\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    str(model_path), monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, \n",
    "    mode='auto', period=1)\n",
    "# Stop when validation loss stops improving\n",
    "early_cb = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "# Save logs to logfile\n",
    "log_path = logs_dir / (model_name + '_' + datetime.datetime.now().isoformat() + '_log.csv')\n",
    "log_cb = keras.callbacks.CSVLogger(str(log_path), separator=',', append=False)\n",
    "\n",
    "history = model.fit(x, y, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.25, \n",
    "                    callbacks=[checkpoint_cb, log_cb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metrics from the log file\n",
    "metrics = pd.read_csv(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([metrics[::10], metrics[-1:]])) # every 10th metric and the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Accuracy \n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.0,1.0]) # Show results on 0..1 range\n",
    "plt.plot(metrics[\"acc\"])\n",
    "plt.plot(metrics[\"val_acc\"])\n",
    "plt.legend(['Training Accuracy', \"Validation Accuracy\"])\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.plot(metrics[\"loss\"])\n",
    "plt.plot(metrics[\"val_loss\"])\n",
    "plt.legend(['Training Loss', \"Validation Loss\"])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Effect of Training on Text Generation\n",
    "\n",
    "Use models from different training epochs to generate text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(model_name, epoch):\n",
    "    model_path = models_dir  /  (model_name + f'_{epoch:02d}.h5')\n",
    "    return model_path\n",
    "\n",
    "\n",
    "def weighted_sample(probs):\n",
    "    '''\n",
    "    probs is a 2d array where each row is a separate probability distribution for the next character\n",
    "    return an index for each row corresponding to a randomly sampled probability.\n",
    "    Example:\n",
    "    [[0.8, 0.1, 0.1],\n",
    "     [0.2, 0.5, 0.3]]\n",
    "    '''\n",
    "    # this has no axis argument\n",
    "    # np.random.choice(len(preds), p=preds)\n",
    "\n",
    "    # https://stackoverflow.com/questions/40474436/how-to-apply-numpy-random-choice-to-a-matrix-of-probability-values-vectorized-s\n",
    "    #cum holds the cumulative distributions:\n",
    "    c = probs.cumsum(axis=1)\n",
    "    # Generate a set of uniformly distributed samples...\n",
    "    u = np.random.rand(len(c), 1)\n",
    "    #...and then see where they \"fit\" in c:\n",
    "    choices = (u < c).argmax(axis=1)\n",
    "    return choices\n",
    "        \n",
    "    \n",
    "def max_sample(probs):\n",
    "    return np.argmax(probs, axis=-1)\n",
    "\n",
    "\n",
    "def seed_text(text, seq_len):\n",
    "    start = np.random.randint(0, len(text) - seq_len)\n",
    "    return text[start:(start + seq_len)]\n",
    "\n",
    "\n",
    "def generate_text_for_epochs(model_name, epochs, text, seq_len, vocab_size, num_samples, sample_len):\n",
    "    for epoch in epochs:\n",
    "        path = get_model_path(model_name, epoch)\n",
    "        model = keras.models.load_model(path)\n",
    "        print('Epoch {}:'.format(epoch))\n",
    "        for i in range(num_samples):\n",
    "            seed, sample = generate_text(model, text, seq_len, vocab_size, sample_len)\n",
    "            print(sample)\n",
    "\n",
    "\n",
    "def generate_text(model, text, seq_len, vocab_size, output_len):\n",
    "    int_to_char = datagen.get_int_to_char(vocab_size)\n",
    "    char_to_int = datagen.get_char_to_int(vocab_size)\n",
    "    # initial sequences to prime the generation of next characters\n",
    "    seed = seed_text(text, seq_len)\n",
    "    # as tensors for input to model.  shape (1, seq_len, vocab_size)\n",
    "    x_seq = seed\n",
    "    output = ''\n",
    "    # generate output_len characters\n",
    "    for i in range(output_len):\n",
    "        x = datagen.sequences_to_tensor([x_seq], seq_len, char_to_int)\n",
    "        preds = model.predict(x)[0] # shape (1, vocab_size)\n",
    "        idx = np.random.choice(len(preds), p=preds)\n",
    "        char = int_to_char[idx]\n",
    "        output += char\n",
    "        x_seq = x_seq[1:] + char\n",
    "    return seed, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text_for_epochs(model_name, [20, 40, 60, 80, 100], text, SEQ_LEN, VOCAB_SIZE, NUM_GEN_TEXT_SAMPLES, GEN_SAMPLE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
