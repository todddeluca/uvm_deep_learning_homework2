{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Explore how to get the data into a more suitable format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import projd\n",
    "import sys\n",
    "import nltk\n",
    "import nltk.data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# for importing local code\n",
    "src_dir = str(Path(projd.cwd_token_dir('notebooks')) / 'src') # $PROJECT_ROOT/src\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "import config\n",
    "import load\n",
    "\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnp_text = load.load_pride_and_prejudice()\n",
    "jokes = load.load_jokes()\n",
    "names = load.load_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pride and Prejudice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pnp has 31 header lines before the book title and 366 footer lines after the end of the book.\n",
    "# remove the project gutenberg header and footer\n",
    "lines = list(pnp_text.splitlines())[31:-366]\n",
    "# print the beginning and end of the book\n",
    "for i, l in enumerate(lines[:10] + lines[-10:]):\n",
    "    print(i, l)\n",
    "\n",
    "text = ' '.join(lines)\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "sentences = sent_detector.tokenize(text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Approximate number of sentences:', len(sentences))\n",
    "print('Number of words:', len(text.split()))\n",
    "print('Number of characters:', len(' '.join(text.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in enumerate(sentences[:100]):\n",
    "    print(i, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnpdf = pd.DataFrame(sentences, columns=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnpdf.sentence.str.len()\n",
    "pnpdf['len'] = pnpdf.sentence.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pnp_into_sentences():\n",
    "    pnp_text = load.load_pride_and_prejudice()\n",
    "    # pnp has 31 header lines before the book title and 366 footer lines after the end of the book.\n",
    "    # remove the project gutenberg header and footer\n",
    "    lines = list(pnp_text.splitlines())[31:-366]\n",
    "    text = ' '.join(lines)\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sent_detector.tokenize(text.strip())\n",
    "    pnp = pd.DataFrame(sentences, columns=['text'])\n",
    "    return pnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine distribution of text lengths in jokes, pnp, and names\n",
    "\n",
    "The typical way to train an RNN is to feed it texts of the same length (padded as needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnp = preprocess_pnp_into_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnp['len'] = pnp.text.str.len()\n",
    "print('max len', pnp.len.max())\n",
    "plt.hist(pnp.len, bins=100)\n",
    "plt.show()\n",
    "print(pnp.shape)\n",
    "pnp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes['len'] = jokes.body.str.len()\n",
    "print(\"max len\", jokes.len.max())\n",
    "plt.hist(jokes.len, bins=100)\n",
    "plt.show()\n",
    "print(jokes.shape)\n",
    "jokes.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names['len'] = names.name.str.len()\n",
    "print(\"max len\", names.len.max())\n",
    "plt.hist(names.len, bins=100)\n",
    "plt.show()\n",
    "print(names.shape)\n",
    "print(names[names.sex=='male'].shape)\n",
    "print(names[names.sex=='female'].shape)\n",
    "names.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How large is each dataset when limited to text of a certain length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for maxlen in [10000, 1000, 400, 200, 100]:\n",
    "    print('maxlen', maxlen)\n",
    "    print('names:', names[names.len < maxlen].shape[0])\n",
    "    print('jokes:', jokes[jokes.len < maxlen].shape[0])\n",
    "    print('pnp:', pnp[pnp.len < maxlen].shape[0])\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Split Datasets\n",
    "\n",
    "For each dataset:\n",
    "  create one long string\n",
    "  lowercase it\n",
    "  divide the string into overlapping substrings of length seqlen, with a stride of s.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequences(text):\n",
    "    '''\n",
    "    normalize text by first lowercasing it and then splitting\n",
    "    it on text on whitespace and recombine the tokens using a \n",
    "    single space char.\n",
    "    \n",
    "    divide the normalized text into sequences of length config.seq_len by \n",
    "    striding it in strides of length stride_len and taking the subsequence \n",
    "    of length seq_len at that position.\n",
    "\n",
    "    return: list of sequences\n",
    "    '''\n",
    "    normalized_text = ' '.join(text.split()).lower()\n",
    "    sequences = [normalized_text[i:(i + config.seq_len)] \n",
    "                 for i in range(0, len(normalized_text), config.stride_len)]\n",
    "    return sequences\n",
    "\n",
    "def preprocess_pnp2():\n",
    "    '''\n",
    "    return: dataframe with 'text' column and 'category' column.\n",
    "    '''\n",
    "    pnp_text = load.load_pride_and_prejudice()\n",
    "    # pnp has 31 header lines before the book title and 366 footer lines after the end of the book.\n",
    "    # remove the project gutenberg header and footer\n",
    "    text = ' '.join(' '.join(list(pnp_text.splitlines())[31:-366]).lower().split())\n",
    "    print('num chars:', len(text))\n",
    "    sequences = text_to_sequences(text)\n",
    "    pnp = pd.DataFrame(sequences, columns=['text'])\n",
    "    pnp['category'] = 'pride'\n",
    "    return pnp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnp = preprocess_pnp2()\n",
    "pnp.text.iloc[:10].apply(repr)\n",
    "pnp.head()\n",
    "pnp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_jokes():\n",
    "    '''\n",
    "    return: dataframe with 'text' column and 'category' column.\n",
    "    '''\n",
    "    jokes = load.load_jokes()\n",
    "    text = ' '.join(' '.join(jokes.body).lower().split())\n",
    "    print('num chars:', len(text))\n",
    "    sequences = text_to_sequences(text)\n",
    "    df = pd.DataFrame(sequences, columns=['text'])\n",
    "    df['category'] = 'jokes'\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes = preprocess_jokes()\n",
    "jokes.text.iloc[:10].apply(repr)\n",
    "jokes.head()\n",
    "jokes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_names():\n",
    "    '''\n",
    "    return: dataframe with 'text' column and 'category' column.\n",
    "    '''\n",
    "    names = load.load_names()\n",
    "    male_text = ' '.join(' '.join(names[names.sex == 'male'].name).lower().split())\n",
    "    print('num chars male_text:', len(male_text))\n",
    "    male_sequences = text_to_sequences(male_text)\n",
    "    male_df = pd.DataFrame(male_sequences, columns=['text'])\n",
    "    male_df['category'] = 'male_names'\n",
    "    female_text = ' '.join(' '.join(names[names.sex == 'female'].name).lower().split())\n",
    "    print('num chars female_text:', len(female_text))\n",
    "    female_sequences = text_to_sequences(female_text)\n",
    "    female_df = pd.DataFrame(female_sequences, columns=['text'])\n",
    "    female_df['category'] = 'female_names'\n",
    "    print('num_chars:', len(male_text) + len(female_text))\n",
    "    return pd.concat([female_df, male_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = preprocess_names()\n",
    "names.text.iloc[:10].apply(repr)\n",
    "names.head()\n",
    "names[names.category == 'male_names'].head()\n",
    "names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
